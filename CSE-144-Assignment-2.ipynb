{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"R3SW8S-xw-ja"},"source":["# CSE 144 Fall 2023 HW 2\n","\n","In this assignment, your goal is to build a logistic regression classifier on a simple synthetic dataset (included in train.csv and test.csv).\n","\n","## Instruction \n","\n","- Submit your assignments onto **Canvas** by the due date.\n","- This is an **individual** assignment. All help from others (from the web, books other than text, or people other than the TA or instructor) must be clearly acknowledged. \n","- Don't change the input and output structure of pre-defined functions. Most coding parts can be finished with about 5-6 lines of codes. \n","- Remember that tuning on the test loss is prohibited.\n","\n","## Rubric\n","\n","The assignment is worth 65 points in total:\n","- Data (10 points)\n","- Model (45 points)\n","    - initialization (5 points)\n","    - gradient descent (5 points)\n","    - sigmoid (5 points)\n","    - cross-entropy loss (5 points)\n","    - derivative cross-entropy loss (5 points)\n","    - accuracy (5 points)\n","    - Train (10 points)\n","    - Evaluate (5 points)\n","- Trainer (10 points)\n","    - Define trainer and train (5 points)\n","    - Evaluate (5 points)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DF5DHsvoaqik"},"source":["# Dataset preparation\n","In the cell below, you will read training and test data. You should spile the dataset into features and labels for each of training, validation, and test sets. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1197,"status":"ok","timestamp":1647069864170,"user":{"displayName":"Chris Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQemSXNy3-8lMOGHAoMnKl5GiUQPYaVv5RZoPlxi4=s64","userId":"18245379379241780097"},"user_tz":480},"id":"O3qPeXtUw76-","outputId":"a24a4dad-b3e4-46a2-eef3-515562b2028f"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","%config InlineBackend.figure_format=\"retina\"\n","import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","\n","\n","def plot_data(x: np.ndarray, y: np.ndarray) -> None:\n","    \"\"\"\n","    Plot a dataset with 2-d feature vectors and binary labels. \n","\n","    Args:\n","        x: 2-d feature vectors\n","        y: 1-d binary labels.\n","    \"\"\"\n","    class0_idx = np.where(y == 0)[0]\n","    class1_idx = np.where(y == 1)[0]\n","    feature0 = x[:, 0]\n","    feature1 = x[:, 1]\n","    plt.scatter(feature0[class0_idx], feature1[class0_idx], label=\"0\")\n","    plt.scatter(feature0[class1_idx], feature1[class1_idx], label=\"1\")\n","    plt.legend()\n","    plt.show()\n","\n","\n","def plot_decision_boundary(theta, x) -> None:\n","    \"\"\"\n","    Plot the decision boundary using theta. Use this function with plot_data().\n","\n","    Args:\n","        theta: a 3-d weight vector.\n","        x: 2-d feature vectors, which is used to decide the span of the decision\n","           boundary.\n","    \"\"\"\n","    xx = np.linspace(min(x[:, 0]), max(x[:, 0]))\n","    yy = (-theta[1] / theta[2]) * xx - (theta[0]) / theta[2]\n","    plt.plot(xx, yy, color=\"red\", label=\"boundary\")\n","    plt.ylim(min(x[:, 1]), max(x[:, 1]))\n","\n","\n","# Read datasts and split your training data into train & validation sets. Split\n","# features from labels after that.\n","# ========== YOUR CODE STARTS HERE ==========\n","\n","# ========== YOUR CODE ENDS HERE ==========\n","print(x_train.shape, y_train.shape)\n","print(x_val.shape, y_val.shape)\n","print(x_test.shape, y_test.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"p9uz4oGuau6p"},"source":["## Plot training and validation sets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1647069864171,"user":{"displayName":"Chris Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQemSXNy3-8lMOGHAoMnKl5GiUQPYaVv5RZoPlxi4=s64","userId":"18245379379241780097"},"user_tz":480},"id":"iyxYPXoxv-Xm","outputId":"6891c5bd-ba9c-42a9-8f96-290cbe4de03b"},"outputs":[],"source":["x_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":515},"executionInfo":{"elapsed":523,"status":"ok","timestamp":1647069864686,"user":{"displayName":"Chris Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQemSXNy3-8lMOGHAoMnKl5GiUQPYaVv5RZoPlxi4=s64","userId":"18245379379241780097"},"user_tz":480},"id":"pQ_9N-r9a7Mp","outputId":"d7b9dd37-609f-4780-d29b-073b268f8c72"},"outputs":[],"source":["plot_data(x_train, y_train)\n","plot_data(x_val, y_val)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pqOAWFA_axTP"},"source":["# Logistic regression\n","You'll complete the missing parts in the eight functions in the `LogisticRegressionTrainer` class below. Note that you are not supposed to return anything in `gradient_descent_step()` but update the parameters. Especially, do not forget to add the regularization term in `cross_entropy_loss()`."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":284,"status":"ok","timestamp":1647070448836,"user":{"displayName":"Chris Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQemSXNy3-8lMOGHAoMnKl5GiUQPYaVv5RZoPlxi4=s64","userId":"18245379379241780097"},"user_tz":480},"id":"YZOF6w9lw3zh"},"outputs":[],"source":["class LogisticRegressionTrainer:\n","    def __init__(\n","        self,\n","        num_features: int,\n","        learning_rate: float = 1e-2,\n","        num_epochs: int = 500,\n","        lambd: float = 0.0,\n","    ) -> None:\n","        \"\"\"Initialize a logisitc regression trainer.\"\"\"\n","        self.lambd = lambd\n","        self.learning_rate = learning_rate\n","        self.num_epochs = num_epochs\n","        self.num_features = num_features\n","        self.train_loss_history = []\n","        self.val_loss_history = []\n","        self.train_acc_history = []\n","        self.val_acc_history = []\n","        self.test_loss = None\n","        self.test_acc = None\n","\n","        # Initialize weights for your model. You can use any initialization methods.\n","        # ========== YOUR CODE STARTS HERE ==========\n","        # ========== YOUR CODE ENDS HERE ==========\n","\n","    def gradient_descent_step(self, x: np.ndarray, y: np.ndarray) -> None:\n","        \"\"\"\n","        Perform a single step of gradient update.\n","\n","        Args:\n","            x: A matrix of features.\n","            y: A vector of labels.\n","        \"\"\"\n","        # ========== YOUR CODE STARTS HERE ==========\n","        # ========== YOUR CODE ENDS HERE ==========\n","\n","    def sigmoid(self, z: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Convert raw model output (logits) to probabilities.\n","\n","        Args:\n","            z: Raw model output (logits).\n","\n","        Returns:\n","            A vector (or float, if your input is a scalar) of probabilties.\n","        \"\"\"\n","        # ========== YOUR CODE STARTS HERE ==========\n","        # ========== YOUR CODE ENDS HERE ==========\n","\n","    def cross_entropy_loss(self, pred: np.ndarray, target: np.ndarray) -> float:\n","        \"\"\"\n","        Calculates the binary cross-entropy loss given predictions and targets.\n","        The loss function should add the regularization term.\n","\n","        Args:\n","            pred: Predicted labels (probabilities).\n","            target: Ground-truth labels.\n","\n","        Returns:\n","            A scalar of loss.\n","        \"\"\"\n","        assert pred.shape == target.shape\n","        # ========== YOUR CODE STARTS HERE ==========\n","        # ========== YOUR CODE ENDS HERE ==========\n","\n","    def cross_entropy_loss_derivative(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Calculate the derivative of the loss function w.r.t. theta. The derivative of the\n","        loss function should also add the derivative of the L2 regularization term.\n","\n","        Args:\n","            x: Feature vectors.\n","            y: Ground-truth labels.\n","\n","        Returns:\n","            A vector with the same dimension as theta, where each element is the\n","            partial derivative of the loss function w.r.t. the corresponding element\n","            in theta.\n","        \"\"\"\n","        # ========== YOUR CODE STARTS HERE ==========\n","        # ========== YOUR CODE ENDS HERE ==========\n","\n","    def accuracy(self, pred: np.ndarray, target: np.ndarray) -> float:\n","        \"\"\"\n","        Calculates the percentage of matched labels given predictions and targets.\n","\n","        Args:\n","            pred: Predicted labels (rounded probabilities).\n","            target: Ground-truth labels.\n","\n","        Return:\n","            The accuracy score (a float) given the predicted labels and the true labels.\n","        \"\"\"\n","        assert pred.shape == target.shape\n","        # ========== YOUR CODE STARTS HERE ==========\n","        # ========== YOUR CODE ENDS HERE ==========\n","\n","    def train(\n","        self,\n","        x_train: np.ndarray,\n","        y_train: np.ndarray,\n","        x_val: np.ndarray,\n","        y_val: np.ndarray,\n","    ) -> None:\n","        \"\"\"\n","        Run gradient descent for n epochs, where n = self.num_epochs. In every epoch,\n","            1. Update theta.\n","            2. Calculate the training loss & accuracy given the current theta, and append \n","               then to self.train_loss_history and self.train_acc_history.\n","            3. Calculate the validation loss & accuracy given the current theta, and \n","               append then to self.train_loss_history and self.train_acc_history.\n","\n","        If you wish to use the bias trick, please remember to use it before the for loop.\n","\n","        Args:\n","            x_train: Feature vectors for training.\n","            y_train: Ground-truth labels for training.\n","            x_val: Feature vectors for validation.\n","            y_val: Ground-truth labels for validation.\n","        \"\"\"\n","        # ========== YOUR CODE STARTS HERE ==========\n","        # ========== YOUR CODE ENDS HERE ==========\n","\n","    def evaluate(self, x_test: np.ndarray, y_test: np.ndarray) -> None:\n","        \"\"\"\n","        Evaluate the model on test set and store the test loss int self.test_loss and \n","        test accuracy in self.test_acc. In other words, you should get the test loss and accraucy here.\n","\n","        If you used the bias trick in train(), you have to also use it here.\n","\n","        Args:\n","            x_test: Feature vectors for testing.\n","            y_test: Ground-truth labels for testing.\n","        \"\"\"\n","        # ========== YOUR CODE STARTS HERE ==========\n","        # ========== YOUR CODE ENDS HERE =========="]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8DQ5HqSEazeb"},"source":["## Train a logistic regression classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":675},"executionInfo":{"elapsed":1558,"status":"ok","timestamp":1647070451241,"user":{"displayName":"Chris Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQemSXNy3-8lMOGHAoMnKl5GiUQPYaVv5RZoPlxi4=s64","userId":"18245379379241780097"},"user_tz":480},"id":"1knVC-6KkTuV","outputId":"2e8206c3-bc20-4355-b63e-618a1595e654"},"outputs":[],"source":["# Train a logistic regression classifier\n","# ========== YOUR CODE STARTS HERE ==========\n","# ========== YOUR CODE ENDS HERE ==========\n","\n","print(f\"Final train loss: {trainer.train_loss_history[-1]}\")\n","print(f\"Final validation loss: {trainer.val_loss_history[-1]}\")\n","print(f\"Final train acc: {trainer.train_acc_history[-1]}\")\n","print(f\"Final validation acc: {trainer.val_acc_history[-1]}\")\n","\n","plt.plot(np.arange(trainer.num_epochs), trainer.train_loss_history, label=\"Train loss\")\n","plt.plot(np.arange(trainer.num_epochs), trainer.val_loss_history, label=\"Val loss\")\n","plt.title(\"Train & validation loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()\n","\n","plt.plot(np.arange(trainer.num_epochs), trainer.train_acc_history, label=\"Train acc\")\n","plt.plot(np.arange(trainer.num_epochs), trainer.val_acc_history, label=\"Val acc\")\n","plt.title(\"Train & validation acc\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lmXGufdLcTbD"},"source":["## Plotting decision boundaries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":550},"executionInfo":{"elapsed":869,"status":"ok","timestamp":1643739816723,"user":{"displayName":"Chris Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQemSXNy3-8lMOGHAoMnKl5GiUQPYaVv5RZoPlxi4=s64","userId":"18245379379241780097"},"user_tz":480},"id":"At9nduYG2-6w","outputId":"bb109932-0c7e-405f-870b-0d797ec52027"},"outputs":[],"source":["# 0 = theta0 + theta1 * x + theta2 * y\n","# y = (-theta0 - theta1 * x) / theta2\n","print(f\"My logistic regression weights: {trainer.theta}\")\n","plot_decision_boundary(trainer.theta, x_val)\n","plot_data(x_val, y_val)\n","\n","model = LogisticRegression(penalty=\"l2\", n_jobs=-1).fit(x_train, y_train)\n","print(f\"Sklearn logisitic regression weights: {np.append(model.intercept_, model.coef_)}\")\n","plot_decision_boundary(np.append(model.intercept_, model.coef_), x_val)\n","plot_data(x_val, y_val)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6KEuYFqHa4LD"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1643739816724,"user":{"displayName":"Chris Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQemSXNy3-8lMOGHAoMnKl5GiUQPYaVv5RZoPlxi4=s64","userId":"18245379379241780097"},"user_tz":480},"id":"ltBpOW90CFJB","outputId":"d3fb478a-266f-4fcc-aaa9-8cd7981bc8c7"},"outputs":[],"source":["# Evaluate your model on the test set\n","# ========== YOUR CODE STARTS HERE ==========\n","# ========== YOUR CODE ENDS HERE ==========\n","print(f\"Test loss: {trainer.test_loss}\")\n","print(f\"Test acc: {trainer.test_acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PsNphLCphvsD"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"CSE-144-Assignment-2-Solutions.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
